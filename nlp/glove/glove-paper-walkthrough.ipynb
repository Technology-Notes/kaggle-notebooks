{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Paper Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Jeffrey Pennington, Richard Socher, Christopher D. Manning\n",
    "\n",
    "https://nlp.stanford.edu/pubs/glove.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "* Start by defining $X_{ij} = \\text{number of times i appears within the context of j}.$\n",
    "  * To use the notation from other word embedding training models, $i$ is the target and $j$ is the context.\n",
    "  * Context could be defined as 10 words to the left and right of the word.\n",
    "     * In this case, it would be an symmetric relationship: $X_{ij} = X_{ji}$\n",
    "  * Another context definition like the word directly before or after, would a asymentrical.\n",
    "  \n",
    "* Model wants to minimize the following:\n",
    "  * $\\sum\\limits_{i=1}^{10000} \\sum\\limits_{j=1}^{10000} (\\theta_i^T e_j + b_i + b_j - logX_{ij})^2$\n",
    "  \n",
    "  * Also includes a weighting term that has two purposes:\n",
    "     * Set result to 0, if $ij$ never appear within the same context (eg $X_{ij} = 0$).\n",
    "     * Handles weighting frequent and infrequent terms.\n",
    "     \n",
    "* Because of the symmetric relationship, the final weights can be calculated as follows:\n",
    "  * $e_w^{(final)} = \\frac{e_w + \\theta_w}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "* \"*Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque.*\"\n",
    "\n",
    "  * Other algorithms for learning word embedding are good, but hard to interpret.\n",
    "\n",
    "* \"*We analyze and make explicit the model properties needed for such regularities to emerge in word vectors.*\"\n",
    "  * We have figured out how to build a word embedding and explain exactly why it works.\n",
    "\n",
    "* \"*The result is a new global log bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods.*\n",
    "  * [\"A log-bilinear Language Model (LM) computes the probability of the next word wi given the previous words (context)\"](https://www.quora.com/What-is-a-log-bilinear-model)\n",
    "\n",
    "* \"*Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus.*\"\n",
    "  * Skip any word-word cooccurrence that doesn't exist in training.\n",
    "    * Note: word-word cooccurence happens when a target word is found some number of places before or after a context word.\n",
    "\n",
    "* *The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.*\n",
    "  * The vector produced does really well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "### Related Work\n",
    "\n",
    "### The GloVe Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As code\n",
    "\n",
    "The first challenge is building the cooccurance matrix. Basically, for each word in the corpus, you'd create a row in the matrix, that's `(num_words x num_words)` then you'd walk through each other word and increment the position as follows: ``matrix[i][j] += 1; matrix[j][i] += 1``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
